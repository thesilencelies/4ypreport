\section{Going forwards}
There are a number of areas in which this project could be advanced. One, much like with \cite{RVA}, would be to add an extra output value to the agent that would choose whether or not to stop at that iteration. Then the agent could receive a penalty per step taken and learn initially inefficient methods that do produce good results before iteratively improving it's efficiency. This would be particularly useful for more classical optimisation situations, where the precision rather than the computation available is the defined variable.

Another obvious area for improvement would be to modify the deterministic Actor Critic agent for this system so that it actually learns, applying many of the ideas laid out at the end of section~\ref{sec:detrfo}. This may also require significantly more computational power to be made available to the system, or perhaps some form of the parallel learning described in \cite{deepmindparallel}, which looks like it could allow for such learning to be reasonably performed on a high end CPU.

Another clear oversight of this paper, due to a lack of available time, is the absence of any comparison to the industry standard Bayesian optimisers. This is probably the most urgent next step, that of creating Bayesian optimisation agent that can be both used for comparison and for tutoring the apprenticed learner a better policy. The impact for the apprenticed learner is likely to be particularly strong, as unlike with the pattern search, the Bayesian optimiser won't be drawn just into exploiting some local minima, but is much better behaved in terms of exploring other likely locations and reasonably reducing the likelihood of it not finding anything.


\section{Conclusions}