\section{Intial Implimentations}
The initial aim of this project was to produce an agent that could learn to play some video game using reinforcement learning, based on the results from the Google Deepmind Atari paper \cite{ataripaper} %need correct citation
This section details the work done and results from that work.
\subsection{Maze solving agents}
To provide understanding about reinforcement learning and develop familiarity with training neural networks on such tasks, a trivial Maze world was created. In the maze world the agent always has the same four actions - move up, left, right or down. In the world are walls, which if the agent attempts to enter it instead remains where it is, pits, which terminate the episode and give a negative reward, and one goal, which terminates the episode and gives a positive reward.
In all cases the agent followed an epsilon greedy policy.
Monte Carlo methods were found to be unsuitable in such an enviroment due to the fact that there are many non-terminal policies, and even if the episode is forcefully terminated, these tend to cause the agent to excessively penalise various areas, harming the learning. 1 step and TD lambda methods both found optimal policies in the maze very efficiently whilst using a tabular representation of the correct behaviour.
The function approximation used to feed it into the neural network was an indicator function across all locations fed into a deep feedforward neural network. Experience replay and assesment networks were also implimented. However, the problem proved to be more complex than anticipated. It could be observed that the agent was indeeed learning something, however the learning was unstable, and suffered from a plethora of local minima. Although there are many more advanced techniques available to further improve it's behaviour, it was decided to move on from this issue as many of the techniques involved would be tangential to the issues in training to play a card game, and time was limited.