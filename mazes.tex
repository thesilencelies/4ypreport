\section{Intial Implimentations}
The initial aim of this project was to produce an agent that could learn to play some video game using reinforcement learning, based on the results from the Google Deepmind Atari paper \cite{ataripaper} %need correct citation
This section details the work done and results from that work.
\subsection{Maze solving agents}
To provide understanding about reinforcement learning and develop familiarity with training neural networks on such tasks, a trivial Maze world was created. In the maze world the agent always has the same four actions - move up, left, right or down. In the world are walls, which if the agent attempts to enter it instead remains where it is, pits, which terminate the episode and give a negative reward, and one goal, which terminates the episode and gives a positive reward.
In all cases the agent followed an epsilon greedy policy. % these  bits need a lot more explaination or some more space from the lit review to explain such methods

For comparison and understanding, simple tabular agents were created for various tabular paradgims as well. With these, it was found that Monte Carlo methods (whereby updates are only done upon episode termination and are done using the exact reward) are unsuitable in such an enviroment due to the fact that there are many non-terminal policies. Even when the episode is forcefully terminated, these tend to cause the agent to excessively penalise various areas, harming the learning. However 1 step and TD lambda methods (which use a weighted aggregate of the reward over the future steps) both found optimal policies in the maze very efficiently whilst using a tabular representation of the correct behaviour.

The function approximation used to feed it into the neural network was an indicator function across all locations fed into a deep feedforward neural network. Experience replay and assesment networks were also implimented. However, the problem proved to be more complex than anticipated. It could be observed that the agent was indeeed learning something, however the learning was unstable, and suffered from a plethora of local minima. Although there are many more advanced techniques available to further improve it's behaviour, it was decided to move on from this issue as many of the techniques involved would be tangential to the issues in training to play a card game, and time was limited.