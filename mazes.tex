\section{Initial Implementations}
The initial aim of this project was to produce an agent that could learn to play some video game using reinforcement learning, based on the results from the Google Deepmind Atari paper \cite{atariDQN} %need correct citation
This section details the work done and results from that work.
\subsection{Maze solving agents}
To provide understanding about reinforcement learning and develop familiarity with training neural networks on such tasks, a trivial Maze world was created. In the maze world the agent always has the same four actions - move up, left, right or down. In the world are walls, which if the agent attempts to enter it instead remains where it is, pits, which terminate the episode and give a negative reward, and one goal, which terminates the episode and gives a positive reward.
In all cases the agent followed an epsilon greedy policy.

For comparison and understanding, simple tabular agents were created for various tabular paradigms as well, where the agent stores a separate value for each location in the maze. With these, it was found that Monte Carlo methods (whereby updates are only done upon episode termination and are done using the exact reward) are unsuitable in such an environment due to the fact that there are many non-terminal policies. Even when the episode is forcefully terminated, these tend to cause the agent to excessively penalise various areas, harming the learning. 1 step (which perform the bellman update after each step using the current estimate for the next value, a process known as bootstrapping) and TD lambda methods (which use a weighted aggregate of the reward over all the future steps, typically implemented using eligibility traces) both found optimal policies in the maze very efficiently whilst using a tabular representation of Q and V.

%explain a bit more here

The function approximation used was deep feed forward neural network given as input a matrix whose value was zero at all points except the one corresponding to the current location. Experience replay and assessment networks were also implemented. However, the problem proved to be more complex than anticipated. It could be observed that the agent was indeed learning something, however the learning was unstable, and suffered from a plethora of local minima. Although there are many more advanced techniques available to further improve its behaviour, it was decided to move on from this issue as many of the techniques involved would be tangential to the issues in training to play a card game, and time was limited.