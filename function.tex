\section{Function Optimization}
%what we tried, why we tried it, and why it didn't work
%define prior here
Given the vast state complexity present within MtG, an alternative avenue of research with potentially more useful applications was suggested. The task was to train some agent to be able to, given some black box function $f(\boldsymbol{x})$, find $\underset{\boldsymbol{x}_i}{\text{argmin}} (f(\boldsymbol{x}_i))$. Current methods that are used for such situations either require a very large number of iterations (pattern searching, simplex method) or some form of prior for the expected shape of the function (Bayesian optimisation). So, if an agent could be trained to compute the minima within a small number of steps with no explicit prior, that could be useful in a number of cases.

The particular gains of such an agent would probably be most apparent in situations where there is strong, but unknown, similarity between the functions. This is because the agent, if it is to outperform the all purpose methods, is likely to learn an implicit prior over the functions it is trained on. So if it is trained exclusively on functions of the type that it will be used on, then theoretically it can produce better results for such systems without need to define an explicit prior.

Another interesting result from this experiment would be to see what sort of behaviour the agent learns - how does it balance exploration versus exploitation? Does it attempt to perform newton steps or similar to find the lowest point? This could demonstrate better what types of computation is preferred by neural networks of this type. By getting the agent to occasionally save the output to disk, the set of points explored and the values they returned can be analysed, and the behaviour of the agent better understood as well.


This could be defined as  a Markov decision process, where the action is either to trial some $\boldsymbol{x}_i in f(\boldsymbol{x}_i)$ or stop, the state is set of previous observations of $f(\boldsymbol{x}_i)$ and the reward is: 
\begin{equation*}
r = \begin{cases}
 step penalty&  \text{non-terminal} \\
-Loss(f(\boldsymbol{x}), f(\boldsymbol{x}_{min})) & \text{ terminal}
\end{cases}
\end{equation*}
 $Loss(a,b)$ is some function that is at a minimum when $a = b, \forall a \geq b$ and $step penalty$ is some non-positive constant that encourages the agent to reach a minimum in the smallest number of steps. 
In order to define the problem in such a way that it can learn reasonably and fair comparisons could be done it was decided that it was known (or constrained to be) that the minima would lie within some known finite subspace of $\mathbb{R}^n$. In practice this means that the search space and minima were constrained by $x_i \leq x_{max}$ where $x_{max}$ is some known constant.

%a diagram could help, also use maths. like, less words, more Ms
The reward scheme that simply rewards it for the difference between the final return value and the optimum was chosen not only because of its simplicity, but also because of its independence from the x coordinate checked. Under schemes where the x coordinate needs to be close to the minimum x coordinate, it gets penalised for following optimal behaviour into an unfortunate local minimum. Take the example of a function with two minima, the global one,$M_g$ at $x_{min}$ and a local one $M_l$ at $x_{local}$, where $x_{min}$ and $x_{local}$ are far apart. Further suppose that $f(x_{min_l}) = f(x_{min_g}) + \epsilon$ where $\epsilon$ is small. Suppose the agent explored nearby to both of these minima, and happened to observe a point $\hat x_l$ that is closer  enough to the local minima that $f(\hat x_l) < f(\hat x_g)$, where $\hat x_g$ is the closest point observed to the global minima. This means the agent would return $\hat x_l$ as the minima. Under a reward scheme based on x that would be heavily penalised, due to the large distance in x from the global, despite being entirely logical. So a reward scheme based  purely on $f(x)$ is desirable.

For the experimentation, a series of polynomial functions were defined so that their parameters could be passed as an input, allowing existing neural network training architectures to be used. Each polynomial was defined by randomly choosing a set of roots from within the search space, then producing a series of coefficients by multiplying out
$\int \prod_i (x - r_i) dx$, where $r_i$ is the ith root. Where $\boldsymbol{x}$ has multiple dimensions, in each dimension a separate polynomial is defined this way, so that $f(\boldsymbol{x})  = \sum_i \text{Poly}_i(x_i)$ where $\text{Poly}_i(x)$ is the polynomial function for the ith dimension. Then $f(\boldsymbol{x}$ is evaluated at every combination of roots, and the one with the lowest value is the global minima. The full algorithm is detailed in figure~\ref{alg:functiongen}.

\begin{figure}
\begin{algorithmic}
\State Randomly select  $N_{dim}\times N_{roots}$ values within range [$-x_{max}, x_{max}$] as the roots

\Function {expandTerms}{inds, maxind, numloops, d}
            \If {numloops = 0}
               \State val = 1
                \For {each ind stored in inds}
                    \State$ val \gets val*ind$
                \EndFor
               \State \Return{$val$}
            \Else
                \State $val = 0$
                \For{ i = maxind, numloops, -1}
                    \State inds[numloops] = -roots[i][d]  \Comment{roots correspond to (x - a) terms}
                    \State$ val \gets val $+ expandTerms(inds, i-1,numloops -1, d) 
                \EndFor
                \State \Return{$val$}
            \EndIf
      \EndFunction
        
     \For{ j = 1, ndim}
        \For{ i = 1, nroots}
            \State params[j][i] = loopick(\{\}, nroots, i, j)
        \EndFor
    \EndFor
   \State find min by checking all roots
    
  \State params used to make function of x thus:
  \Function F{coords, params}
    \State out = 0
    \For {i = 1, dim}
         \State res = 1/(npar + 1) 
        \State x = coords[i]
        \For {j = 1, npar} 
              \State res = res * x + (params[i][j] /(npar +1 -j))
	    \EndFor
	   \State out =  out + (res*x)
    \EndFor

    \Return out
\EndFunction
\end{algorithmic}
\caption{algorithm to generate the functions}
\label{alg:functiongen}
\end{figure}

Given the variance of these polynomials, and in particular how much the reward changes with higher orders or dimensions, it was necessary to define a more even comparison between the architectures. One useful statistic was the error rate, defined as the proportion of final values that lay more that 5\% of the average absolute value of the minima away from the global minimum. This indicates how many were ``close enough'' to the target. To further normalize things, two baseline agents were created to give a scale for the rewards to be put on. For simplicity of comparison, the number of steps the agent could take was fixed, and $steppenalty$ was set to 0. The baseline agents were a brute force agent that simply divided the search space into equal blocks and looked across all of these, ignoring the values it received. The reward this equal search agent received was defined as 0 relative reward. The other agent uses pattern search, where it checks a grid around the current best location, moves to the new best if there is one, or reduces the grid size if there isn't. The reward this pattern search agent achieved was set as 1 relative reward.

It was decided that, rather than get the agent to learn how to make the comparisons internally (or potentially learn to interpolate between observed values) that the minimum value observed overall would be worked out externally and fed to the neural network as an additional variable. The idea behind this decision is that this frees up the learning in the network to be devoted to finding the best behaviour for the system, rather than having to additionally use up neurons storing this value and performing the comparison in a potentially harmful manner. This does remove the opportunity for the agent to learn how to guess what the true minimum would be based on its observations, but it was decided that such a behaviour would be extremely prone to over fitting and the gains from it would not be worth the cost in terms of the additional learning overheads. Furthermore such an output would simply learn to return lower numbers under the current reward scheme, which means that the reward would also need to consider the x value of the optimum, which had already been discarded.

\subsection{Recurrent Function Optimisation}
The first design that was attempted was based on the work in Recurrent Models of Visual Attention\cite{RVA}. It used the overall structure shown in figure~\ref{fig:RFOarch} The idea is that the internal state of the recurrent neural network would be able to describe the state so that the feed-forwards network can decide what location to look at next. The network would be trained directly with REINFORCE, so only the rewards are needed, not any critic or similar structure. The final output is the minimum value observed, which is tracked at each function evaluation step, and also passed to the RNN to help describe the state space better, as then it doesn't have to learn to do that as well. The recurrence for the architecture that was designed can be seen in fig~\ref{fig:RFOarch}. On top of that structure, the average reward $b$ was also learned as a parameter of the network so that the variance reduction in REINFORCE could be used. One crucial difference is that, unlike with the visual attention paper \cite{RVA}, there is no classification, so no classification loss to train the RNN with, so it is only being trained by the REINFORCE module.

\begin{figure}
\centering
\input{diagrams/RecurrentArch}
\caption{Architecture for recurrent function optimisation}
\label{fig:RFOarch}
\end{figure}

Initially it was very unstable, only wanting to search values at the boundaries of the search space. It seemed to be the case that the internal parameters were exploding to massive values and saturating on every pass. So two normalisation hyper parameters were used - the cutoffNorm and the maxOutNorm. The cutoff norm is the maximum L2 norm of all of the gradients of the parameters for all the layers. If the norm exceeds that value, all the gradients are scaled down by the same amount so that the L2 norm for the gradients equals the cutoffNorm. This prevents exploding gradients within the recurrent elements of the network. The maxOutNorm sets the maximum L2 norm of any one layer of the network. Then, like with the cutoffNorm, if the L2 norm for the layer exceeds the maxOutNorm then all of the parameters of that layer are scaled down by the same amount until their L2 norm is that value. This, like weight decay, allows the network to ``forget'' unhelpful learning and also keeps the outputs bounded. Once both of these were applied the agent began to use much more of the space.

Another important trick that significantly improved performance was normalising the inputs to the network. Particularly in the presence of the above parameters, it is necessary to have every input value to the network to be of the same order of magnitude. However, the range of the output of the function isn't known exactly, and its relative magnitude to itself is very important for working out the minimum. The idea that was hit upon to standardise the outputs was to use an approximate upper bound on the output of the function and divide it by that. Because of how the function was defined, the highest order term of the polynomial always has a coefficient of 1 (as any other function could be scaled to be like that anyway). Furthermore, given that the roots are within known bounds, the agent will never ask for $x$ values above those bounds. So if the output of the function approximator is divided by $x_{max}^{p+1}$, where $p$ is the number of parameters used to define the function, then the vast majority of the outputs should lie within the range [$0 - 1$]. Given that actually the agent spends a lot more time within tighter bounds than those, and the other inputs were in the range [$0- x_{max} $] the output was actually divided by $x^p$. This did result in immediate improvement in performance.

Two different forms for the loss function were tried - $Loss(f(\boldsymbol{x}), f(\boldsymbol{x}_{min})) = log(f(\boldsymbol{x}) - f(\boldsymbol{x}_{min} +1))$ and $Loss(f(\boldsymbol{x}), f(\boldsymbol{x}_{min})) = f(\boldsymbol{x}) - f(\boldsymbol{x}_{min})$. The log form was proposed because it was noted that the linear loss produced potentially very large gradients, and there were concerns about stability. However, it seems that the log loss massively slowed learning down, and the agent seemed to stabilize to some policy relatively quickly. This is probably because the ideal loss function curve should be linear with the error for large error, whilst the log term is less than that.

One other variation that was attempted to try and improve the results, based on how the agent in the visual attention paper \cite{RVA} was trained, was to calculate what the reward would be at every step, and use the gradient based on these to train the agent instead. The results are labelled everystep below, and generally seemed to do worse. The key difference seems to be that the output is based on the lowest observed value, rather than its current estimate of the truth at each step, so it ended up producing large penalties for what were potentially reasonable exploratory steps, and so producing more nuisance gradients that drove the policy away from optimality.

The exact setup chosen for the experimentation, in terms of number of layers and location of non-linearities, is detailed in figure~\ref{fig:exactsetup}. Each white rectangle is a fully connected layer with size equal to the variable written on it. 
\begin{figure}
\centering
\input{diagrams/exactsetup}
\caption{Layerwise setup for all of the RFO agents}
\label{fig:exactsetup}
\end{figure}

\subsubsection{Experimental Method}
%need to do this
For the experimentation, the implementation of the agent was ran on a single GPU for 250 epochs, and tested on the test data after each epoch. There were  163840 training examples generated and 20480 test examples. 

There were four main experiments performed: a comparison of the impact of number of hidden units in the network to performance, a comparison between using ReLU and HardTanh for the transfer functions, a comparison of giving the reward at every step or only the first, and comparison of the agent's performance with functions of different dimensions.

Unless otherwise specified, the functions the agent was learning on were a series of 1 dimensional quartics, defined as 1D3P (because it has 3 stationary points). They were all ran with a fixed number of steps, 20, except for with the dimensionality increases, where the number of steps increases by 10 for each additional dimension.

\subsubsection{RFO Results}
In each of the charts, the normalised performance on the set of 20480 test functions is used. So a value of 0 indicates that the agent received the same average reward across the tests set as an agent that simply divides the search space evenly, while a score of 1, marked by the line labelled pattern search, indicates that the agent achieved the same average reward as a pattern search agent on the training data. The aim for this project would be to produce an agent who can produce a normalised score significantly higher than 1. Each chart lists the agents  normalised reward on the test data as a function of training epoch, so that both final result and learning rate can be examined.


In figure~\ref{fig:opt1full} the comparison between the agent's performance on the training and the validation data can be seen. Unlike with many supervised learning problems, the two seem well coupled, if both subject to some degree of stochasticity. The final accuracies obtained are somewhat disappointing, though as can be seen in figure~\ref{fig:opt1norm} the error rate at best does reach that of the pattern search agent. Also visible in figure~\ref{fig:opt1norm} is that the performance on the testing and validation data sets are near identical. Due to this, in all further graphs, only the test reward is considered.

\begin{figure}
\centering
\input{graphs/opt1-full}
\caption{RFO learning behaviour \\ \emph{These values are not normalised as they include training values}}
\label{fig:opt1full}
\end{figure}
\begin{figure}
\centering
\input{graphs/opt1-full-norm}
\caption{RFO normalised performance across learning}
\label{fig:opt1norm}
\end{figure}

%I should probably only use on of the two hyperparam setups

%detail the numbers used in the experiments and explain what the graphs mean - you can do this now

From figure~\ref{fig:locexp} the chronological behaviour of the agent becomes apparent. It checks a spread of locations around the best point observed so far, both those close so that it can get a better value, but also those progressively further away in case there is a better minimum there. Although the behaviour seems slightly random, in the second path it returns to an area that clearly contains points that are worse those it had found, the policy performs very well, consistently getting a normalised score of 1.8, with an test error rate of less than 1\%. Interestingly the agent doesn't seem to be performing any form of gradient based search, but seems to be taking steps in 

\emph{there will be data analysis here once I have the data - I'm retaking because I get much better results when I change one of the parameters}


%detail exactly how these experiments were ran. - what was changed, by how much etc.
\begin{figure}
\centering
\input{graphs/demopoints}
\caption{Points checked by the agent and their return values}
\label{fig:locexp}
\end{figure}

%\begin{figure}
%\centering
%\input{graphs/Exp1-sizes}
%\caption{Comparison of Agent learning for different hidden sizes}
%\label{fig:exp1rfo}
%\end{figure}
%
%\begin{figure}
%\centering
%\input{graphs/Exp1-sizes-newp}
%\caption{Comparison of Agent learning for different hidden sizes with alternative hyper parameters}
%\label{fig:exp1rfo2}
%\end{figure}

\begin{figure}
\centering
\input{graphs/Exp1-sizes-long}
\caption{Comparison of Agent learning for different hidden sizes}
\label{fig:exp1rfo}
\end{figure}

\subsubsection{Issues and Evaluation} %needs more here

%like, a lot more

Two key problems seem to be hampering the behaviour of this agent. One is that it is very easy for it to get stuck in local minima, whereby the local gradient of the policy is zero, but it is not at the optimum policy. Although that can be reduced by having a larger training dataset, it cannot be avoiding within this architecture. This problem is further exacerbated by the fact that there is nothing guiding the formation of the internal state except for the reinforce, which means that potentially it isn't retaining various pieces of salient information that would allow it to make better decisions.

It should be noted that in general recurrent neural networks are particularly susceptible to local minima, as they produce chaotic responses to changes in the error surface\cite{Cuéllar2006}, and this was particularly apparent during testing, where sometimes the agent would simply fail to find anything useful during training. %give example if you have one

It was found out during the first round of experimentation that the early stopping value was probably too low, as the agent would spend a long time with a low reward that wasn't changing very much until suddenly it would burst forth with a large improvement, and the tight early stopping boundary was preventing larger networks from being able to reach that point.

That all said, the agent does manage to learn a policy that consistently outperforms the pattern search with several of the networks in figure~\ref{fig:exp1rfo}, and as the number of dimensions increases, %we'll find out.

\begin{figure}
\centering
\begin{minipage}{.8\textwidth}
\begin{algorithmic}
\State $S(\boldsymbol{O}, \boldsymbol{s}_{i-1}; \theta) \mapsto \boldsymbol{s}_i$
\State $Q(\boldsymbol{s} ;\theta) \mapsto \boldsymbol{x} $
\State $G(\mu,\sigma)$  \Comment{Gaussian Noise}
\State choose some initial $b$ \Comment{Baseline reward}
 \Repeat
 	\State Pick some initial $\boldsymbol{x}_i$
 	\Repeat
 		\State observe $f(\boldsymbol{x}_i)$
 		\If $f(\boldsymbol{x}_i) < f(\hat{\boldsymbol{x}}_{min})$
 			\State$ \hat{\boldsymbol{x}}_{min} \gets \boldsymbol{x}_i$
 		\EndIf
 		\State $\boldsymbol{O}_i = \{f(\boldsymbol{x}_i),\boldsymbol{x}_i, f(\hat{\boldsymbol{x}}_{min}), \hat{\boldsymbol{x}}_{min}\} $
 		\State $s_{i+1} = S(\boldsymbol{O}_i, \boldsymbol{s}_{i}; \theta)$
 		\State $\boldsymbol{x}_{i+1} = G(Q(\boldsymbol{s}_{i+1};\theta),\sigma)$ 
 	\Until{Max steps}
	\State $R = f(\hat{\boldsymbol{x}}_{min}) - f(\boldsymbol{x}_min)$
	\State $b \gets b  + \alpha (R - b)$ \Comment{MSE gradient step for $b = \mathbb{E}[R]$}
	\State $\delta = (R - b) \alpha \nabla_\theta \text{log}(Q(\boldsymbol{s}_{i+1})$
	\State Update $\theta$ in the direction of $\delta$ using backpropagation through time
 \Until{Max epochs}
 \end{algorithmic}
 \end{minipage}
 \caption{Algorithm for running and training the Recurrent Function Optimizer}
 \label{alg:rfo}
\end{figure}

\subsection{Apprenticed RFO}

%discuss how fair the number of RL steps given is or otherwise

%Why don't they converge to the pattern reward?

One suggestion to improve the performance, based on the work in Mastering the game of Go with deep neural networks and tree search\cite{alphaGo}, was to train the agent to first replicate ``expert'' output, then further improve the policy using reinforcement learning as before. The idea is that by first moving to a space where it is making good actions already, it would escape local minima and have a more defined manner in which it would learn to define the state space. The first key challenge with this is to choose what sort of agent it should learn from. Initially it was proposed to get it to learn from a simplex agent, but the implementations of simplex agents within the systems constraints were found to be consistently outperformed by the pattern search agent. So it was decided to use the pattern search agent instead.

One possible advantage of this set up for training the agent is that it makes training the agent for an expensive task without a decent model for it a lot easier. Instead of either training the agent directly on the expensive task, which would take a very long time and lots of computation, instead output data from previously used optimisers could be used to train the agent along with some loose approximation based on the data from those, which should reduce how much computational time the agent has to spend on ``live'' examples.

For each training function the agent produced an output, and in parallel the pattern search agent produced its own output. Mean squared error loss was used to produce the gradient, where the error was defined as the difference between the output of the agent for that step and the pattern searcher.

In order for it to produce adequate output to match the performance of the tutor, a number of hyper parameters had to be adjusted as well, which was useful for tuning the hyper parameters for the less well defined straight RL case.

\subsubsection{Apprenticed RFO results}
The same set of experiments was ran with the apprenticed RFO as with the pure reinforcement learned agent. %compare both how well it does internally, and to RFO
The graphs are normalised in the same way, and the pattern search level is again indicated. Additionally, the step where the apprenticeship was turned off and the agent swapped to reinforcement learning is marked on each chart with a dot at that point.

\emph{there will be data analysis here once I have the data}
\begin{figure}
\centering
\input{graphs/Exp1-sizes-long-app}
\caption{Comparison of Apprenticed RFO learning for different hidden sizes}
\label{fig:exp1apprfo}
\end{figure}

%
%\begin{figure}
%\centering
%\input{graphs/Exp1-sizes-app-newp}
%\caption{Comparison of Apprenticed RFO learning for different hidden sizes with new parameters}
%\label{fig:exp1apprfo}
%\end{figure}

\subsubsection{Issues and Evaluation}
 However, there are several unusual trends worth pointing out. For larger networks, it seems that the actual rewards received during apprenticeship actually decreased as it went along, though it is worth remembering that the reward is not the parameter it was learning to optimise during that stage. This is probably because with the larger networks the initial pass produces something that spreads out quite evenly across the space, then it learns to focus in, but tends to over fit, resulting in poor rewards.

The other interesting trend is the agent learned to produce something similar to the output after a reasonable number of iterations, at which point it was achieving only slightly worse scores than the pattern search. Then when it was switched to reinforcement learning it produced a temporary drop in performance, followed by a significant improvement, exceeding that of the pure RL agent. However this result seems to be unstable, as upon further iterations it produces decaying performance. The improvement in performance of the agent after the initial batch could be attributed to it learning to explore better, as in all the cases the function used was non-convex, so the pattern search will often end up in the worse of the two minima.

\begin{table}[hbtp]
\centering
\begin{tabular}{r | cc}
Hidden Size & RFO & Apprenticed \\
\hline
16 & 2.23 & 0.26 \\
32 & 2.47 & 1.89 \\
64 & 2.07 & 1.98\\
128 & 2.66 & 2.40 \\
256 & 2.76 & 2.10 \\
512 & 2.71 & 2.37 \\
1024 & 1.58 & \\

\end{tabular}

%need more tables here for the other experiments 

\caption{Best normalised rewards for each of the networks}
\label{tab:compare}

\end{table}•

As can be seen in table~\ref{tab:compare}

\subsection{Deterministic RFO}
\label{sec:detrfo}
Given that the environment is both stochastic and non-adversarial, the optimal policy for the agent should be deterministic. So REINFORCE can only ever asymptotically converge to the optimal policy, and given the actual implementation, not even that. So the methods described in Continuous control with deep reinforcement learning\cite{lillicrap2015continuous} ought to be able to produce better performance. A deterministic actor critic agent was created, within the same code structure as the REINFORCE agents, but this one is, in theory, off policy. The total structure of the agent is detailed in figure~\ref{fig:detrfo}. The key additions are the critic networks, which consist of both an RNN that produces an observation of the state, that is the series of value-coordinate pairs so far observed, and the Q-network, which estimates the value of that state.
\begin{figure}
\centering
\input{diagrams/QLearnArch}
\caption{Deterministic RFO structure}
\label{fig:detrfo}
\end{figure}

So far the learning with this has been much slower, though more stable than that with REINFORCE, it seems to also get stuck in an unprofitable local minima really easily, as can be seen in figure~\ref{fig:detpolgrad}. Given more time, the next step would be to find the hyperparameter setup that gives it the fastest learning rate whilst continuing being stable and running it with the apprenticeship method to get it away from the worse of the local minima. With the apprenticeship method, the network would be trained by error compared to the tutor whilst the critic learns from the experiences as well so that both are well initialised when it switches to reinforcement learning. However time constraints prevent this from happening.

\begin{figure}
\centering
\input{graphs/detpolgrad}
\caption{Performance of RFO with deterministic policy gradient}
\label{fig:detpolgrad}
\end{figure}

